# SwiFT_v2 Benchmark Models - Master Index

**Purpose**: Central reference for benchmark models and comparison framework
**Date**: October 23, 2025
**Status**: ‚úÖ Complete - Ready for Evaluation

---

## Overview

SwiFT_v2 will be evaluated against two state-of-the-art brain foundation models:

1. **Brain-JEPA** - Joint-Embedding Predictive Architecture for brain dynamics
2. **BrainLM** - Masked Autoencoder for brain activity recordings

This index provides quick navigation to all benchmark-related documentation and code.

---

## Quick Reference

### Brain-JEPA
- **Paper**: Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking
- **Venue**: NeurIPS 2024 (Spotlight)
- **ArXiv**: https://arxiv.org/abs/2409.19407
- **Architecture**: Vision Transformer + JEPA (Non-contrastive learning)
- **Key Innovation**: Gradient-based positioning + Spatiotemporal masking
- **ROIs**: 450 (400 Schaefer cortical + 50 Tian subcortical)
- **Embedding Dim**: 768-1024
- **Pretraining Data**: UK Biobank
- **Evaluated On**: UKB, HCP-Aging, ADNI, MACC

### BrainLM
- **Paper**: BrainLM: A Foundation Model for Brain Activity Recordings
- **Venue**: OpenReview
- **GitHub**: https://github.com/vandijklab/BrainLM
- **Architecture**: Vision Transformer + MAE (Masked Autoencoder)
- **Key Innovation**: Scalable masked prediction, transfer learning
- **Model Variants**: 111M parameters (efficient), 650M parameters (high-capacity)
- **Pretraining Data**: UK Biobank
- **Evaluated On**: UKB, ADNI, EMBARC, YooAttn, ToPS

### SwiFT_v2
- **Status**: [Under Evaluation]
- **Architecture**: [To be compared]
- **Parameters**: [To be measured]
- **Embedding Dim**: [To be measured]
- **Pretraining Data**: [Document]

---

## Documentation Structure

### Analysis Documents
1. **`BRAIN_JEPA_ANALYSIS.md`** (12 sections, comprehensive)
   - Technical contributions
   - Data specifications
   - Model architecture details
   - Training methodology
   - Code structure and embedding extraction
   - Strengths and limitations
   - Relevance for SwiFT_v2

2. **`BRAINLM_ANALYSIS.md`** (15 sections, comprehensive)
   - Technical contributions
   - Data and datasets
   - Model architecture
   - Training methodology
   - Downstream capabilities
   - Code structure
   - Model variants and usage
   - Strengths and limitations
   - Relevance for SwiFT_v2

3. **`BENCHMARK_COMPARISON_FRAMEWORK.md`** (10 sections, operational)
   - Comparison dimensions
   - Evaluation protocols
   - Benchmark datasets
   - Metrics summary
   - Experimental protocols
   - Implementation checklist
   - Output deliverables
   - Manuscript integration
   - Timeline and success criteria

4. **`DATASETS_FOR_EVALUATION.md`** (üìã Dataset specifications)
   - ADNI dataset (primary evaluation dataset)
   - HCP-Aging dataset (secondary evaluation dataset)
   - Paper-derived dataset information
   - Data access and preprocessing guidelines
   - Quality control standards
   - Integration with benchmark framework

5. **`DOWNSTREAM_EVALUATION_TASKS.md`** (üìã NEW - Task definitions)
   - Task 1: MCI-to-AD Conversion Prediction (neurodegenerative domain)
   - Task 2: Antidepressant Response Prediction (psychiatric domain)
   - Task 3: Pain-Evoked Brain State Classification (functional domain)
   - Clinical significance and evaluation protocols
   - Implementation workflow and expected outcomes
   - Unified benchmark suite specifications

---

## Key Comparison Dimensions

### Architecture
| Aspect | Brain-JEPA | BrainLM |
|--------|-----------|---------|
| Base | JEPA | MAE |
| Masking | Spatiotemporal (structured) | Random patches |
| Positional Encoding | Gradient-based | Standard |
| Output Layer | Global pooling | CLS token |
| Parameter Count | [Estimate] | 111M / 650M |

### Training
| Aspect | Brain-JEPA | BrainLM |
|--------|-----------|---------|
| Objective | Prediction loss | Reconstruction loss |
| Pretraining Data | UK Biobank | UK Biobank |
| Hardware | 4x A100 (40GB) | Flexible |
| Loss Type | Non-contrastive | Contrastive/Masked |

### Evaluation
| Task | Brain-JEPA | BrainLM |
|------|-----------|---------|
| Linear Probe | HCP-Aging (sex), ADNI (disease) | Multiple datasets |
| Fine-tuning | Yes | Yes |
| Zero-shot | Not reported | Tutorial provided |
| KNN Baseline | Not reported | Available |

---

## Project Code Locations

### Brain-JEPA
- **Project Root**: `/Users/apple/Desktop/Brain-JEPA`
- **Key Scripts**:
  - `downstream_embedding_extraction.py` - Extract embeddings from pretrained model
  - `src/datasets/external_dataset_parcellation.py` - Prepare external data
  - `src/models/vision_transformer.py` - ViT architecture
- **Documentation**: `EMBEDDING_EXTRACTION_GUIDE.md` (detailed extraction protocol)
- **Modified For**: Extracting embeddings from pretrained checkpoints

### BrainLM
- **Project Root**: `/Users/apple/Desktop/BrainLM`
- **Key Scripts**:
  - `brainlm_mae/modeling_brainlm.py` - Core model class
  - `brainlm_mae/configuration_brainlm.py` - Configuration
  - `continue_train_same_wandb.py` - Training script
- **Notebooks**:
  - `brainlm_tutorial.ipynb` - Data preprocessing
  - `inference_02_cls_token_knn_regressor.ipynb` - KNN evaluation
  - `inference_03_cls_token_mlp_classification.ipynb` - Classification
- **Model Hub**: https://huggingface.co/vandijklab/brainlm/
- **Modified For**: Embedding extraction pipeline

---

## Evaluation Datasets

### Primary: ADNI
- **Disease**: Alzheimer's (NC vs MCI vs AD)
- **Subjects**: [Number]
- **Task**: Disease classification
- **Relevance**: Both models evaluated here; primary comparison point
- **Status**: [To be prepared]

### Secondary: HCP-Aging
- **Task**: Sex classification
- **Subjects**: [Number]
- **Relevance**: Brain-JEPA evaluated here; out-of-distribution test
- **Status**: [To be prepared]

### Tertiary: SwiFT_v2 Internal Datasets
- **Status**: [To be documented]

---

## Experimental Workflow

```
Step 1: Setup
‚îú‚îÄ Create benchmark dataset structure
‚îú‚îÄ Download and prepare ADNI data
‚îú‚îÄ Prepare train/val/test splits
‚îî‚îÄ Compile metadata and labels

Step 2: Brain-JEPA Evaluation
‚îú‚îÄ Load pretrained checkpoint
‚îú‚îÄ Extract embeddings (ADNI)
‚îú‚îÄ Linear probing
‚îú‚îÄ Fine-tuning
‚îú‚îÄ Representation analysis
‚îî‚îÄ Save results and embeddings

Step 3: BrainLM Evaluation
‚îú‚îÄ Load from HuggingFace (111M and 650M)
‚îú‚îÄ Extract embeddings (ADNI)
‚îú‚îÄ Linear probing
‚îú‚îÄ Fine-tuning
‚îú‚îÄ KNN-based evaluation
‚îú‚îÄ Representation analysis
‚îî‚îÄ Save results and embeddings

Step 4: SwiFT_v2 Evaluation
‚îú‚îÄ Extract embeddings (same data)
‚îú‚îÄ Run same downstream tasks
‚îú‚îÄ Compute comparison metrics
‚îî‚îÄ Save results and embeddings

Step 5: Analysis & Comparison
‚îú‚îÄ Compile results table
‚îú‚îÄ Perform statistical testing
‚îú‚îÄ Generate visualizations
‚îú‚îÄ Analyze representations
‚îî‚îÄ Document findings

Step 6: Manuscript Integration
‚îú‚îÄ Create benchmark section
‚îú‚îÄ Write results and discussion
‚îú‚îÄ Generate figures for paper
‚îî‚îÄ Prepare supplementary materials
```

---

## Key Metrics to Compute

### Performance Metrics
- Linear probing accuracy
- Fine-tuning accuracy
- AUC-ROC
- Sensitivity/Specificity
- Data efficiency curves

### Efficiency Metrics
- Model parameters
- Embedding dimensions
- Inference time per subject
- Peak GPU memory
- Pretraining time

### Representation Metrics
- Embedding correlation
- Preserved brain structure
- Dimensionality (PCA)
- KNN classification quality
- Nearest neighbor overlap

---

## Analysis Summaries

### Brain-JEPA: Key Strengths
‚úÖ Novel gradient-based positional encoding
‚úÖ Structured spatiotemporal masking (multi-scale)
‚úÖ Non-contrastive learning (efficient, no collapse)
‚úÖ NeurIPS 2024 spotlight recognition
‚úÖ Clear code and documentation
‚úÖ Embedding extraction pipeline available

### Brain-JEPA: Key Limitations
‚ö†Ô∏è Designed for specific ROI atlas (Schaefer + Tian)
‚ö†Ô∏è High computational cost for pretraining (4x A100)
‚ö†Ô∏è Temporal downsampling (490 ‚Üí 160 frames)
‚ö†Ô∏è UK Biobank-specific pretraining

### BrainLM: Key Strengths
‚úÖ Scalable variants (111M and 650M)
‚úÖ MAE approach (simple, interpretable)
‚úÖ HuggingFace integration (easy loading)
‚úÖ Multiple downstream task examples
‚úÖ KNN-based no-training baseline
‚úÖ Tested on diverse datasets (ADNI, EMBARC, YooAttn, ToPS)

### BrainLM: Key Limitations
‚ö†Ô∏è Flash Attention complexity for 650M model
‚ö†Ô∏è Preprocessing varies by dataset
‚ö†Ô∏è Fine-tuned models not yet released
‚ö†Ô∏è Less neuroscience-specific design

---

## Relevance for SwiFT_v2 Paper

### What These Benchmarks Establish
1. **State-of-the-art Baselines**: Current best performance on brain fMRI
2. **Architecture Diversity**: Two different approaches (JEPA vs MAE)
3. **Scale Range**: Covers 111M to 650M parameters
4. **Evaluation Best Practices**: Standardized downstream tasks

### SwiFT_v2 Positioning
- Compare to cutting-edge foundation models
- Demonstrate advantages/tradeoffs
- Position in landscape of brain foundation models
- Support claims with empirical evidence

### Expected Paper Sections
- **Introduction**: Brief overview of benchmarks
- **Methods**: How comparison conducted (datasets, metrics)
- **Results**: Performance tables, efficiency comparison
- **Discussion**: SwiFT_v2 advantages/disadvantages vs benchmarks
- **Supplementary**: Extended analysis, visualizations

---

## Next Steps

### Immediate (This Week)
1. ‚úÖ Complete paper and code analysis (DONE)
2. ‚úÖ Create benchmark documentation (DONE)
3. ‚è≥ Prepare datasets and directory structure
4. ‚è≥ Extract Brain-JEPA embeddings
5. ‚è≥ Extract BrainLM embeddings (111M and 650M)

### Short-term (Next 2 weeks)
1. ‚è≥ Extract SwiFT_v2 embeddings
2. ‚è≥ Run downstream task evaluations
3. ‚è≥ Compute comparison metrics
4. ‚è≥ Perform statistical analysis

### Medium-term (Next month)
1. ‚è≥ Analyze representations
2. ‚è≥ Generate comparison visualizations
3. ‚è≥ Write manuscript section
4. ‚è≥ Prepare final benchmark report

---

## Files in This Directory

```
benchmarks/
‚îú‚îÄ‚îÄ BENCHMARKS_INDEX.md                    # This file
‚îú‚îÄ‚îÄ BRAIN_JEPA_ANALYSIS.md                 # Brain-JEPA paper analysis
‚îú‚îÄ‚îÄ BRAINLM_ANALYSIS.md                    # BrainLM paper analysis
‚îú‚îÄ‚îÄ BENCHMARK_COMPARISON_FRAMEWORK.md      # Evaluation framework
‚îú‚îÄ‚îÄ results/                                # [Results directory - TBD]
‚îÇ   ‚îú‚îÄ‚îÄ adni_comparison.csv                # [Results table]
‚îÇ   ‚îú‚îÄ‚îÄ hcp_aging_comparison.csv           # [Results table]
‚îÇ   ‚îî‚îÄ‚îÄ visualizations/                    # [Plots and figures]
‚îî‚îÄ‚îÄ embeddings/                             # [Embeddings storage - TBD]
    ‚îú‚îÄ‚îÄ brain-jepa/
    ‚îú‚îÄ‚îÄ brainlm-111m/
    ‚îú‚îÄ‚îÄ brainlm-650m/
    ‚îî‚îÄ‚îÄ swift-v2/
```

---

## Related Documentation

### In neuro-ai-research-system
- `.claude/HYPOTHESIS_ENGINE_ACADEMIC_INTEGRATION.md` - How benchmarks integrate with hypothesis engine
- `.claude/SYSTEM_INTEGRATION_STATUS.md` - Overall system architecture

### In project directories
- `/Users/apple/Desktop/Brain-JEPA/EMBEDDING_EXTRACTION_GUIDE.md` - Brain-JEPA embedding extraction
- `/Users/apple/Desktop/BrainLM/brainlm_tutorial.ipynb` - BrainLM usage tutorial

---

## Key Papers to Reference

1. **Brain-JEPA**: Dong et al. (2024)
   - Citation: @article{BrainJEPA, title={Brain-JEPA: Brain Dynamics Foundation Model...}, journal={NeurIPS 2024}, year={2024}}

2. **BrainLM**: Vandijk Lab (2024)
   - Citation: @article{BrainLM, title={BrainLM: A Foundation Model for Brain Activity Recordings}, journal={OpenReview}, year={2024}}

3. **I-JEPA**: Assran et al. (2023)
   - Foundation for Brain-JEPA's architecture

4. **MAE**: He et al. (2021)
   - Foundation for BrainLM's architecture

---

## Contact and Resources

### Brain-JEPA
- GitHub: Official repository with code
- Paper: https://arxiv.org/abs/2409.19407
- Model Checkpoints: Google Drive link in README

### BrainLM
- GitHub: https://github.com/vandijklab/BrainLM
- Model Hub: https://huggingface.co/vandijklab/brainlm/
- Lab: Vandijk Lab (Yale University)

---

**Master Index Version**: 1.0
**Created**: October 23, 2025
**Status**: ‚úÖ Complete - Ready for Benchmark Implementation
**Next Review**: After Phase 1 setup completion

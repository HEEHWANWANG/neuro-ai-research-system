# Benchmark Models for SwiFT_v2 Evaluation

**Purpose**: Comprehensive framework for evaluating SwiFT_v2 against state-of-the-art brain foundation models
**Status**: âœ… Documentation Complete - Ready for Benchmark Implementation
**Created**: October 23, 2025

---

## Quick Navigation

### ğŸ“š Documentation Files

| Document | Purpose | Audience |
|----------|---------|----------|
| **README.md** | This file - Overview and navigation | Everyone starting here |
| **BENCHMARKS_INDEX.md** | Master index with quick reference | Quick lookup and navigation |
| **BRAIN_JEPA_ANALYSIS.md** | Detailed Brain-JEPA technical analysis | Implementation and writing |
| **BRAINLM_ANALYSIS.md** | Detailed BrainLM technical analysis | Implementation and writing |
| **BENCHMARK_COMPARISON_FRAMEWORK.md** | Evaluation protocol and metrics | Researchers and analysts |
| **PROJECT_ORGANIZATION.md** | Project structure and workflow | Project managers and coordinators |
| **PHASE_1_DATA_PREPARATION.md** | Dataset preparation guide | Data engineers |

### ğŸ¯ Find What You Need

```
I want to...

â†’ Understand what these benchmark models are?
  Start: BENCHMARKS_INDEX.md â†’ QUICK REFERENCE

â†’ Set up the benchmark evaluation workflow?
  Start: PROJECT_ORGANIZATION.md â†’ WORKFLOW INTEGRATION

â†’ Prepare datasets for evaluation?
  Start: PHASE_1_DATA_PREPARATION.md

â†’ Extract embeddings from Brain-JEPA?
  Start: BRAIN_JEPA_ANALYSIS.md (Section 7) â†’ Brain-JEPA/EMBEDDING_EXTRACTION_GUIDE.md

â†’ Extract embeddings from BrainLM?
  Start: BRAINLM_ANALYSIS.md (Section 7) â†’ BrainLM/brainlm_tutorial.ipynb

â†’ Implement the evaluation protocol?
  Start: BENCHMARK_COMPARISON_FRAMEWORK.md (Section 3 & 5)

â†’ Write the benchmark section for the paper?
  Start: BENCHMARK_COMPARISON_FRAMEWORK.md (Section 8)
```

---

## The Benchmark Models

### ğŸ§  Brain-JEPA
**Paper**: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking
**Venue**: NeurIPS 2024 (Spotlight)
**Architecture**: JEPA-based with brain gradient positional encoding
**Innovation**: Spatiotemporal masking + gradient-based positioning
**Key Stats**: 450 ROI parcellation, 768-1024 embedding dimension

**Quick Facts**:
- âœ… Non-contrastive learning (efficient, no collapse)
- âœ… Neuroscience-specific positional encoding
- âœ… Multi-scale temporal and spatial masking
- âš ï¸ High computational cost (4x A100)
- âš ï¸ Dataset-specific (UK Biobank pretraining)

**Best For**: Models prioritizing brain structure preservation and interpretability

### ğŸ§¬ BrainLM
**Paper**: A Foundation Model for Brain Activity Recordings
**Venue**: OpenReview
**Architecture**: MAE-based with scalable variants
**Innovation**: Masked autoencoder adapted for fMRI sequences
**Key Stats**: 111M and 650M parameter variants, flexible architecture

**Quick Facts**:
- âœ… Scalable variants (111M lightweight, 650M high-capacity)
- âœ… HuggingFace integration (easy loading)
- âœ… Multiple downstream task examples
- âœ… Tested on diverse datasets
- âš ï¸ Flash Attention complexity for 650M
- âš ï¸ Fine-tuned models not yet released

**Best For**: Models balancing performance and computational efficiency

### ğŸš€ SwiFT_v2
**Status**: Under Evaluation
**Role**: Candidate model to benchmark against state-of-the-art

---

## Evaluation Framework

### ğŸ“Š The Three Datasets

| Dataset | Type | Task | Size | Key Advantage |
|---------|------|------|------|---------------|
| **ADNI** | Clinical (Alzheimer's) | 3-way disease classification (NC/MCI/AD) | ~900 subjects | Primary comparison point - both benchmarks evaluated here |
| **HCP-Aging** | Healthy aging | Binary sex classification | ~1000 subjects | Out-of-distribution test - transfers to healthy population |
| **SwiFT_v2 Internal** | Custom | Task-specific | Variable | Internal validation on specialized data |

### ğŸ“ˆ The Evaluation Protocol

```
Phase 1: Setup & Data Preparation (1-2 days)
â”œâ”€ Organize dataset directories
â”œâ”€ Collect demographics and labels
â”œâ”€ Create fixed train/val/test splits (seed=42)
â””â”€ Validate data integrity

Phase 2: Brain-JEPA Evaluation (2-3 days)
â”œâ”€ Extract embeddings (ADNI + HCP-Aging)
â”œâ”€ Linear probing evaluation
â”œâ”€ Fine-tuning evaluation
â”œâ”€ Representation analysis
â””â”€ Save all results

Phase 3: BrainLM Evaluation (2-3 days)
â”œâ”€ Extract embeddings (111M and 650M variants)
â”œâ”€ Run all downstream tasks
â”œâ”€ Compute efficiency metrics
â””â”€ Save all results

Phase 4: SwiFT_v2 Evaluation (1-2 days)
â”œâ”€ Extract embeddings (same data/protocol)
â”œâ”€ Run identical downstream tasks
â”œâ”€ Compute comparison metrics
â””â”€ Save results in standardized format

Phase 5: Analysis & Comparison (2-3 days)
â”œâ”€ Compile comparison table
â”œâ”€ Perform statistical significance testing
â”œâ”€ Generate visualization plots
â”œâ”€ Analyze computational efficiency
â””â”€ Write comprehensive comparison report

Phase 6: Manuscript Integration (1-2 days)
â”œâ”€ Create benchmark results section
â”œâ”€ Generate publication-quality figures
â”œâ”€ Write interpretation and discussion
â””â”€ Prepare supplementary materials
```

### ğŸ¯ Evaluation Tasks

**Task 1: Linear Probing** (Frozen Encoder)
- Train linear classifier on frozen embeddings
- Measures representation quality without fine-tuning
- Fast baseline for model comparison

**Task 2: Fine-tuning** (Full Network)
- Optimize full model on downstream task
- Measures transfer learning capability
- Shows potential with task-specific adaptation

**Task 3: KNN Baseline** (No Training)
- k-Nearest Neighbor classification on embeddings
- Pure representation quality metric
- No training required (BrainLM provides examples)

**Task 4: Efficiency Analysis**
- Model parameters and memory usage
- Inference time per subject
- Training time comparisons
- Power and resource requirements

### ğŸ“Š Comparison Metrics

**Performance Metrics**:
- Linear probing accuracy
- Fine-tuning accuracy
- AUC-ROC
- Sensitivity/Specificity
- Data efficiency curves

**Efficiency Metrics**:
- Model parameters
- Embedding dimensions
- Inference time (ms/subject)
- Peak GPU memory
- Pretraining time

**Representation Metrics**:
- Embedding correlation across models
- Wasserstein distance
- Nearest neighbor overlap
- Brain structure preservation
- Dimensionality (PCA)

---

## Directory Structure

```
.claude/benchmarks/
â”‚
â”œâ”€â”€ README.md                                    # This file
â”œâ”€â”€ BENCHMARKS_INDEX.md                          # Master reference
â”œâ”€â”€ BRAIN_JEPA_ANALYSIS.md                       # Brain-JEPA technical details
â”œâ”€â”€ BRAINLM_ANALYSIS.md                          # BrainLM technical details
â”œâ”€â”€ BENCHMARK_COMPARISON_FRAMEWORK.md            # Evaluation protocol
â”œâ”€â”€ PROJECT_ORGANIZATION.md                      # Project structure & workflow
â”œâ”€â”€ PHASE_1_DATA_PREPARATION.md                  # Dataset preparation guide
â”‚
â”œâ”€â”€ datasets/                                    # Benchmark evaluation datasets
â”‚   â”œâ”€â”€ ADNI/                                   # Primary dataset (Alzheimer's)
â”‚   â”‚   â”œâ”€â”€ raw/                                # Original NIfTI files
â”‚   â”‚   â”œâ”€â”€ brain-jepa/                         # Brain-JEPA embeddings
â”‚   â”‚   â”œâ”€â”€ brainlm-111m/                       # BrainLM 111M embeddings
â”‚   â”‚   â”œâ”€â”€ brainlm-650m/                       # BrainLM 650M embeddings
â”‚   â”‚   â”œâ”€â”€ swift-v2/                           # SwiFT_v2 embeddings
â”‚   â”‚   â”œâ”€â”€ metadata/                           # Demographics and labels
â”‚   â”‚   â””â”€â”€ downstream_results/                 # Evaluation results
â”‚   â”‚
â”‚   â””â”€â”€ HCP_Aging/                              # Secondary dataset (healthy aging)
â”‚       â”œâ”€â”€ raw/                                # Original NIfTI files
â”‚       â”œâ”€â”€ [same structure as ADNI]
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ embeddings/                                  # Central embedding storage
â”‚   â”œâ”€â”€ brain-jepa/                             # Brain-JEPA extracted embeddings
â”‚   â”‚   â”œâ”€â”€ adni/
â”‚   â”‚   â””â”€â”€ hcp_aging/
â”‚   â”œâ”€â”€ brainlm-111m/                           # BrainLM 111M embeddings
â”‚   â”œâ”€â”€ brainlm-650m/                           # BrainLM 650M embeddings
â”‚   â””â”€â”€ swift-v2/                               # SwiFT_v2 embeddings
â”‚
â”œâ”€â”€ results/                                     # Benchmark comparison results
â”‚   â”œâ”€â”€ adni_comparison.csv                     # ADNI results table
â”‚   â”œâ”€â”€ hcp_aging_comparison.csv                # HCP-Aging results table
â”‚   â”œâ”€â”€ efficiency_metrics.csv                  # Computational efficiency
â”‚   â”œâ”€â”€ comparison_metrics.csv                  # Unified metrics table
â”‚   â”œâ”€â”€ visualizations/                         # Publication-quality plots
â”‚   â”‚   â”œâ”€â”€ performance_comparison.png
â”‚   â”‚   â”œâ”€â”€ efficiency_tradeoff.png
â”‚   â”‚   â”œâ”€â”€ representation_analysis.png
â”‚   â”‚   â””â”€â”€ correlation_matrices.png
â”‚   â””â”€â”€ statistical_tests/                      # Statistical analysis
â”‚       â”œâ”€â”€ significance_tests.csv
â”‚       â””â”€â”€ effect_sizes.csv
â”‚
â””â”€â”€ metadata/                                    # Cross-dataset metadata
    â”œâ”€â”€ all_subjects.csv                        # Unified subject list
    â””â”€â”€ preprocessing_log.txt                   # Data preparation documentation
```

---

## Getting Started

### 1ï¸âƒ£ Understand the Benchmark Models (15 minutes)

Read in this order:
1. **BENCHMARKS_INDEX.md** - Quick reference cards
2. **BRAIN_JEPA_ANALYSIS.md** (Sections 1-3) - What is Brain-JEPA?
3. **BRAINLM_ANALYSIS.md** (Sections 1-3) - What is BrainLM?

### 2ï¸âƒ£ Understand the Evaluation Approach (30 minutes)

1. **BENCHMARK_COMPARISON_FRAMEWORK.md** (Sections 1-3) - Overview and protocol
2. **PROJECT_ORGANIZATION.md** (Sections 1-2) - Project structure
3. **BENCHMARKS_INDEX.md** (Evaluation Datasets, Key Metrics) - Quick reference

### 3ï¸âƒ£ Prepare Datasets (1-2 days)

1. **PHASE_1_DATA_PREPARATION.md** - Detailed implementation guide
2. Organize ADNI dataset (raw NIfTI files)
3. Organize HCP-Aging dataset
4. Create metadata (demographics, labels, splits)
5. Validate data integrity

### 4ï¸âƒ£ Extract Embeddings (4-6 days)

**Brain-JEPA**:
1. Read **BRAIN_JEPA_ANALYSIS.md** (Section 7 & 8)
2. Review `/Users/apple/Desktop/Brain-JEPA/EMBEDDING_EXTRACTION_GUIDE.md`
3. Run embedding extraction on both datasets
4. Store in `.claude/benchmarks/embeddings/brain-jepa/`

**BrainLM**:
1. Read **BRAINLM_ANALYSIS.md** (Section 8)
2. Review `/Users/apple/Desktop/BrainLM/brainlm_tutorial.ipynb`
3. Extract embeddings (111M and 650M variants)
4. Store in `.claude/benchmarks/embeddings/brainlm-*/`

### 5ï¸âƒ£ Evaluate & Compare (2-3 days)

1. **BENCHMARK_COMPARISON_FRAMEWORK.md** (Section 5 & 6) - Evaluation protocol
2. Run downstream tasks (linear probing, fine-tuning, KNN)
3. Compute all comparison metrics
4. Generate comparison tables and plots

### 6ï¸âƒ£ Write Manuscript Section (1-2 days)

1. **BENCHMARK_COMPARISON_FRAMEWORK.md** (Section 8) - Manuscript structure
2. Compile results into paper-quality tables and figures
3. Write interpretation and discussion
4. Integrate into SwiFT_v2 manuscript

---

## External Project References

### Brain-JEPA
- **Location**: `/Users/apple/Desktop/Brain-JEPA`
- **Key Guide**: `EMBEDDING_EXTRACTION_GUIDE.md` (detailed extraction protocol)
- **Main Script**: `downstream_embedding_extraction.py`
- **Status**: Modified for external dataset embedding extraction

### BrainLM
- **Location**: `/Users/apple/Desktop/BrainLM`
- **Key Tutorial**: `brainlm_tutorial.ipynb` (data preprocessing and usage)
- **Inference Examples**: `inference_*.ipynb` (downstream task notebooks)
- **Model Hub**: https://huggingface.co/vandijklab/brainlm/
- **Status**: Modified for embedding extraction

---

## Implementation Checklist

### Phase 1: Setup (1-2 days)
- [ ] Read PHASE_1_DATA_PREPARATION.md
- [ ] Create dataset directory structure
- [ ] Download/access ADNI raw fMRI data
- [ ] Download/access HCP-Aging raw fMRI data
- [ ] Collect demographics and clinical labels
- [ ] Create train/val/test splits (seed=42)
- [ ] Validate all data
- [ ] Document preprocessing applied

### Phase 2: Brain-JEPA (2-3 days)
- [ ] Review Brain-JEPA analysis and extraction guide
- [ ] Verify pretrained checkpoint available
- [ ] Extract embeddings on ADNI
- [ ] Extract embeddings on HCP-Aging
- [ ] Run linear probing evaluation
- [ ] Run fine-tuning evaluation
- [ ] Compute representation metrics
- [ ] Save all results to results/

### Phase 3: BrainLM (2-3 days)
- [ ] Review BrainLM analysis and tutorials
- [ ] Load BrainLM 111M from HuggingFace
- [ ] Load BrainLM 650M from HuggingFace
- [ ] Extract embeddings (111M variant)
- [ ] Extract embeddings (650M variant)
- [ ] Run all downstream tasks
- [ ] Compute efficiency metrics
- [ ] Save all results

### Phase 4: SwiFT_v2 (1-2 days)
- [ ] Extract embeddings (same data/protocol)
- [ ] Run identical downstream tasks
- [ ] Compute all metrics
- [ ] Save results in standardized format

### Phase 5: Analysis (2-3 days)
- [ ] Compile comparison metrics table
- [ ] Perform statistical significance testing
- [ ] Generate visualization plots
- [ ] Compute efficiency tradeoffs
- [ ] Analyze representation quality
- [ ] Write comprehensive report

### Phase 6: Manuscript (1-2 days)
- [ ] Write benchmark results section
- [ ] Create publication-quality figures
- [ ] Write interpretation and discussion
- [ ] Prepare supplementary materials
- [ ] Format citations and references

---

## Key Documents by Purpose

### For Quick Reference
- **BENCHMARKS_INDEX.md** - Quick lookup cards, model overview, code locations
- **README.md** (this file) - Navigation and getting started

### For Implementation
- **BRAIN_JEPA_ANALYSIS.md** - Technical details on architecture, training, code
- **BRAINLM_ANALYSIS.md** - Technical details on architecture, variants, usage
- **PHASE_1_DATA_PREPARATION.md** - Step-by-step dataset preparation

### For Project Management
- **PROJECT_ORGANIZATION.md** - Workflow structure, timeline, success criteria
- **BENCHMARK_COMPARISON_FRAMEWORK.md** - Evaluation protocol and metrics

### For Manuscript Writing
- **BENCHMARK_COMPARISON_FRAMEWORK.md** (Section 8) - Expected paper structure
- **Results CSVs** - Comparison metrics for tables and figures

---

## Common Questions

### Q: Where do I start?
**A**: Read BENCHMARKS_INDEX.md for quick overview, then follow "Getting Started" section above.

### Q: How long will the full evaluation take?
**A**: 9-15 days total:
- Phase 1 (Setup): 1-2 days
- Phase 2 (Brain-JEPA): 2-3 days
- Phase 3 (BrainLM): 2-3 days
- Phase 4 (SwiFT_v2): 1-2 days
- Phase 5 (Analysis): 2-3 days
- Phase 6 (Manuscript): 1-2 days

### Q: Can phases run in parallel?
**A**: Yes! Phase 2 (Brain-JEPA) and Phase 3 (BrainLM) can run simultaneously if you have multiple GPUs.

### Q: Where are the pretrained models?
**A**:
- Brain-JEPA: See EMBEDDING_EXTRACTION_GUIDE.md for checkpoint download
- BrainLM: HuggingFace Hub (vandijklab/brainlm-111m and vandijklab/brainlm-650m)

### Q: What's the random seed for reproducibility?
**A**: Seed=42 is used for all train/val/test splits. Documented in PHASE_1_DATA_PREPARATION.md

### Q: How should I organize results for the paper?
**A**: See BENCHMARK_COMPARISON_FRAMEWORK.md (Section 8) for expected manuscript structure and output format.

---

## Citation

When using these benchmark models in your paper, cite:

```bibtex
@article{BrainJEPA,
  title={Brain-JEPA: Brain Dynamics Foundation Model with Gradient Positioning and Spatiotemporal Masking},
  author={Dong et al.},
  journal={NeurIPS 2024},
  year={2024}
}

@article{BrainLM,
  title={BrainLM: A Foundation Model for Brain Activity Recordings},
  journal={OpenReview},
  year={2024},
  note={Vandijk Lab}
}
```

---

## Status & Timeline

âœ… **Documentation Complete** - All analysis and planning documents created
â³ **Phase 1 Ready** - Dataset preparation guide complete, ready for implementation
ğŸš€ **Estimated Full Evaluation**: 9-15 days from Phase 1 start

---

**Last Updated**: October 23, 2025
**Document Version**: 1.0
**Status**: Ready for Benchmark Implementation
**Next Step**: Start Phase 1 - Prepare benchmark datasets
